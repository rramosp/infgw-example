apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b-instruct
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama3-8b-instruct
  template:
    metadata:
      labels:
        app: vllm-llama3-8b-instruct
    spec:

      containers:
      - args:
        - --model=$(MODEL_ID)
        command:
        - python3
        - -m
        - vllm.entrypoints.openai.api_server
        env:
        - name: MODEL_ID
          value: meta-llama/Meta-Llama-3-8B
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              key: token
              name: hf-secret
        image: vllm/vllm-openai:v0.7.2
        name: vllm
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        
        #  name: metrics
        # readinessProbe:
        #   failureThreshold: 60
        #   httpGet:
        #     path: /health
        #     port: 8000
        #   periodSeconds: 10
        lifecycle:
          preStop:
            # vLLM stops accepting connections when it receives SIGTERM, so we need to sleep
            # to give upstream gateways a chance to take us out of rotation. The time we wait
            # is dependent on the time it takes for all upstreams to completely remove us from
            # rotation. Older or simpler load balancers might take upwards of 30s, but we expect
            # our deployment to run behind a modern gateway like Envoy which is designed to
            # probe for readiness aggressively.
            sleep:
              # Upstream gateway probers for health should be set on a low period, such as 5s,
              # and the shorter we can tighten that bound the faster that we release
              # accelerators during controlled shutdowns. However, we should expect variance,
              # as load balancers may have internal delays, and we don't want to drop requests
              # normally, so we're often aiming to set this value to a p99 propagation latency
              # of readiness -> load balancer taking backend out of rotation, not the average.
              #
              # This value is generally stable and must often be experimentally determined on
              # for a given load balancer and health check period. We set the value here to
              # the highest value we observe on a supported load balancer, and we recommend
              # tuning this value down and verifying no requests are dropped.
              #
              # If this value is updated, be sure to update terminationGracePeriodSeconds.
              #
              seconds: 20
            #
            # IMPORTANT: preStop.sleep is beta as of Kubernetes 1.30 - for older versions
            # replace with this exec action.
            #exec:
            #  command:
            #  - /usr/bin/sleep
            #  - 30
        livenessProbe:
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          # vLLM's health check is simple, so we can more aggressively probe it.  Liveness
          # check endpoints should always be suitable for aggressive probing.
          periodSeconds: 1
          successThreshold: 1
          # vLLM has a very simple health implementation, which means that any failure is
          # likely significant. However, any liveness triggered restart requires the very
          # large core model to be reloaded, and so we should bias towards ensuring the
          # server is definitely unhealthy vs immediately restarting. Use 5 attempts as
          # evidence of a serious problem.
          failureThreshold: 5
          timeoutSeconds: 1
        readinessProbe:
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          # vLLM's health check is simple, so we can more aggressively probe it.  Readiness
          # check endpoints should always be suitable for aggressive probing, but may be
          # slightly more expensive than readiness probes.
          periodSeconds: 1
          successThreshold: 1
          # vLLM has a very simple health implementation, which means that any failure is
          # likely significant,
          failureThreshold: 1
          timeoutSeconds: 1
        # We set a startup probe so that we don't begin directing traffic or checking
        # liveness to this instance until the model is loaded.
        startupProbe:
          # Failure threshold is when we believe startup will not happen at all, and is set
          # to the maximum possible time we believe loading a model will take. In our
          # default configuration we are downloading a model from HuggingFace, which may
          # take a long time, then the model must load into the accelerator. We choose
          # 10 minutes as a reasonable maximum startup time before giving up and attempting
          # to restart the pod.
          #
          # IMPORTANT: If the core model takes more than 10 minutes to load, pods will crash
          # loop forever. Be sure to set this appropriately.
          failureThreshold: 3600
          # Set delay to start low so that if the base model changes to something smaller
          # or an optimization is deployed, we don't wait unnecessarily.
          initialDelaySeconds: 2
          # As a startup probe, this stops running and so we can more aggressively probe
          # even a moderately complex startup - this is a very important workload.
          periodSeconds: 1
          httpGet:
            # vLLM does not start the OpenAI server (and hence make /health available)
            # until models are loaded. This may not be true for all model servers.
            path: /health
            port: http
            scheme: HTTP

        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /data
          name: data

      restartPolicy: Always
      enableServiceLinks: false
      terminationGracePeriodSeconds: 130
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-l4
      volumes:
        - name: data
          emptyDir: {}
        - name: dshm
          emptyDir:
            medium: Memory
        - name: adapters
          emptyDir: {}

---
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: vllm-llama3-8b-adapters
# data:
#   configmap.yaml: |
#       basicllama:
#         name: vllm-llama3.1-8b-instruct
#         port: 8000
#         defaultBaseModel: meta-llama/Llama-3.1-8B-Instruct
#         ensureExist:
#           models:
#           - id: llamachat
# #            source: Kawon/llama3.1-food-finetune_v14_r8
# ---
kind: HealthCheckPolicy
apiVersion: networking.gke.io/v1
metadata:
  name: health-check-policy
  namespace: default
spec:
  targetRef:
    group: "inference.networking.x-k8s.io"
    kind: InferencePool
    name: vllm-llama3-8b-instruct
  default:
    config:
      type: HTTP
      httpHealthCheck:
          requestPath: /health
          port: 8000